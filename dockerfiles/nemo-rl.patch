diff --git a/nemo_rl/algorithms/sft.py b/nemo_rl/algorithms/sft.py
index 6de9ac8..fc232ac 100644
--- a/nemo_rl/algorithms/sft.py
+++ b/nemo_rl/algorithms/sft.py
@@ -137,6 +137,7 @@ def setup(
         shuffle=data_config["shuffle"],
         collate_fn=rl_collate_fn,
         drop_last=True,
+        num_workers=10,
     )

     if last_checkpoint_path is not None:
@@ -151,6 +152,7 @@ def setup(
         shuffle=False,
         collate_fn=rl_collate_fn,
         drop_last=True,
+        num_workers=10,
     )

     # ==========================
diff --git a/nemo_rl/models/policy/megatron_policy_worker.py b/nemo_rl/models/policy/megatron_policy_worker.py
index 0b18cb0..ae45383 100644
--- a/nemo_rl/models/policy/megatron_policy_worker.py
+++ b/nemo_rl/models/policy/megatron_policy_worker.py
@@ -550,6 +550,11 @@ class MegatronPolicyWorker:
                 "https://github.com/NVIDIA/Megatron-LM/blob/1ab876ddc4c1893c76f26d775226a8d1dcdfb3d2/megatron/core/transformer/mlp.py#L174."
             )
         model_cfg.apply_rope_fusion = self.cfg["megatron_cfg"]["apply_rope_fusion"]
+        model_cfg.bias_activation_fusion = self.cfg["megatron_cfg"][
+            "bias_activation_fusion"
+        ]
+        if "layernorm_epsilon" in self.cfg["megatron_cfg"]:
+            model_cfg.layernorm_epsilon = self.cfg["megatron_cfg"]["layernorm_epsilon"]

         checkpoint_config = CheckpointConfig(
             save_interval=100,
@@ -918,9 +923,7 @@ class MegatronPolicyWorker:
                         do_not_average_loss=True,
                     )

-                # Empty unused memory.
-                if self.cfg["megatron_cfg"]["empty_unused_memory_level"] >= 1:
-                    torch.cuda.empty_cache()
+

                 # Update parameters.
                 if not eval_mode:
@@ -949,9 +952,6 @@ class MegatronPolicyWorker:
                 else:
                     skipped_iter = 1

-                # Empty unused memory.
-                if self.cfg["megatron_cfg"]["empty_unused_memory_level"] >= 2:
-                    torch.cuda.empty_cache()

                 if parallel_state.is_pipeline_last_stage(ignore_virtual=True):
                     # keep all microbatch metrics to be normalized later
@@ -1237,8 +1237,6 @@ class MegatronPolicyWorker:
                 # if isinstance(item, torch.Tensor):
                 # self.model.state_dict()[name] = item.detach().to(device="cuda", non_blocking=True, copy=True)

-                gc.collect()
-                torch.cuda.empty_cache()

                 # - self.model is the original reference_model, now on CUDA
                 # - self.reference_model is the original model, now on CPU
@@ -1252,8 +1250,6 @@ class MegatronPolicyWorker:
                 # item = item.detach().to(device="cuda", non_blocking=True, copy=True)
                 # self.model.state_dict()[name] = item

-                gc.collect()
-                torch.cuda.empty_cache()

                 ## re-enable overlap param gather after weight swap
                 if self.should_disable_forward_pre_hook:
@@ -1615,7 +1611,6 @@ class MegatronPolicyWorker:
                     if torch.is_tensor(v) and not v.is_cuda:
                         state[k] = v.to("cuda")

-        torch.cuda.empty_cache()

     @wrap_with_nvtx_name("megatron_policy_worker/offload_before_refit")
     def offload_before_refit(self):
@@ -1645,8 +1640,6 @@ class MegatronPolicyWorker:
                         # Move the tensor to CPU and update the state dictionary
                         state[k] = v.to("cpu")

-        gc.collect()
-        torch.cuda.empty_cache()

         # Print memory stats after offloading
         allocated = torch.cuda.memory_allocated() / (1024**3)  # Convert to GB
@@ -1670,8 +1663,6 @@ class MegatronPolicyWorker:
             del self._held_gather_buffer
             self._held_gather_buffer = None

-        gc.collect()
-        torch.cuda.empty_cache()

         allocated = torch.cuda.memory_allocated() / (1024**3)  # Convert to GB
         reserved = torch.cuda.memory_reserved() / (1024**3)  # Convert to GB
