# GenEvolution

GenEvolution encompasses methods that scale inference time via parallel sampling. The approach entails primarily two methods:


- **GenSelect** is a generative Best-of-N method we introduced in the [OpenReasoning paper](https://arxiv.org/abs/2504.16891), followed by a more focused paper -- [GenSelect: A Generative Approach to Best-of-N](https://arxiv.org/abs/2507.17797). The method essentially uses an LLM to reason over and select the best candidate solution among the N candidates, leveraging LLMs' comparative strengths while scaling efficiently across parallel sampling budgets.

- **GenSythesis** takes in the input candidate solutions and outputs a new solutions with the goal of improving over the input solutions.

## Usage

We support GenEvolution via the [generation pipeline](https://nvidia.github.io/NeMo-Skills/pipelines/generation/).
Pass in the following params for the different GenEvolution modes:

- For GenSelect, `++genevolution=True ++genevolution_config.mode=genselect`
- For GenSynthesis, `++genevolution=True ++genevolution_config.mode=gensynthesis`

We support both offline and online GenEvolution:

- *Offline mode*: The candidate solutions/trajectories have already been generated and can be specified via:
`++genevolution_config.generation_dir=<PATH_TO_GENERATED_DIR>`
- *Online mode*: The candidate solutions need to be generated as part of the generation job.

!!!note
    The GenEvolution pipeline uses the same inference parameters as the generate pipeline. We allow overriding of two key inference config params:

    - `temperature` via `++genevolution_config.temperature=<>`
    - `tokens_to_generate` via `++genevolution_config.tokens_to_generate=<>`



### Common Parameters
- `window_size`: Number of solutions compared in a single GenSelect comparison (typically set to 8). Consider your model's context window size when setting this value (or allow for soft failure via `++server.enable_soft_fail=True`).
- `comparison_key`: The key from the generation output used for comparison (default: `generation`)

### Offline GenEvolution Parameters

These parameters only need to be passed when running offline GenEvolution.

- `generation_dir`: The directory where the *offline* generated solutions are stored. We assume the solutions to be in `output-rs*.jsonl` files.
- `num_initial_solutions`: Number of solutions from the offline generated solutions that are used for GenEvolution.


To specify these variables, say `window_size=16`, pass `++genevolution_config.window_size=16` to the generate/eval pipelines.


## Sample Illustrations


### Online GenEvolution (GenSelect)

In this example, we show how to use GenSelect for [aime25](https://nvidia.github.io/NeMo-Skills/evaluation/natural-math/) with [Qwen/Qwen3-8B](https://huggingface.co/Qwen/Qwen3-8B).

```bash hl_lines="9-10"
ns eval \
  --benchmarks aime25 \
  --cluster local \
  --model Qwen/Qwen3-8B \
  --server_gpus 2 \
  --server_type vllm \
  --output_dir /experiments/qwen3_8b/genselect \
  ++inference.tokens_to_generate=16384 \
  ++genevolution=True \
  ++genevolution_config.mode=genselect \
  ++server.enable_soft_fail=True \
  ++server.context_limit_retry_strategy=reduce_generation
```

The evaluation pipeline would first generate `window_size` solutions (8 by default), and then run genselect with these solutions in the prompt to pick one.
Note that the same model is being used for both solution generation and selection, which we refer to as **Self-GenSelect**.
Currently we only support Self-GenSelect in the online mode.

!!!tip
    The inputs for GenEvolution can consume a lot of tokens, especially for large `window_size` values.
    To avoid running into context length issues, we recommend running these pipelines with `++server.enable_soft_fail=True`, as in the above command.
    To use methods for retrying generation with reduced prompt/length, we recommend trying out [the context reduction strategies](https://nvidia.github.io/NeMo-Skills/pipelines/generation/#context-window-limits) supported.
    In the above example, we use:
    ```++server.enable_soft_fail=True ++server.context_limit_retry_strategy=reduce_generation```
    which reduces the generation budget when context limit exceeds.

### Offline GenEvolution (GenSynthesis)

Offline genevolution breaks down the generation and evolution (selection/synthetis) part into two separate steps. There are two use cases which are currently only supported with offline genselect:

- Using a different model for generation and *evolution*.
- Using a processed version of generated output for *evolution*.

In the following example, we use [Qwen/Qwen3-8B](https://huggingface.co/Qwen/Qwen3-8B) to perform GenSelect over solutions generated by [Qwen/Qwen3-4B](https://huggingface.co/Qwen/Qwen3-4B) for `livecodebench`.

```bash hl_lines="9-12"
ns eval \
  --benchmarks livecodebench:8 \
  --cluster local \
  --model Qwen/Qwen3-8B \
  --server_gpus 2 \
  --server_type vllm \
  --output_dir /experiments/qwen3_4b/gensynthesis_qwen3_8b \
  ++inference.tokens_to_generate=16384 \
  ++genevolution=True \
  ++genevolution_config.mode=gensynthesis \
  ++genevolution_config.generation_dir=/experiments/qwen3_4b/eval-results/livecodebench \
  ++genevolution_config.comparison_key=completion \
  ++server.enable_soft_fail=True \
  ++server.context_limit_retry_strategy=reduce_generation
```

There are three things we want to highlight in the above example:

- We run the GenSynthesis pipeline a total of 8 times (`livecodebench:8`) over the same set of solutions
- The pre-generated solutions are specified via: `++genevolution_config.generation_dir=/experiments/qwen3_4b/eval-results/livecodebench`
- Instead of the usual `generation` key for comparing the different solutions, we use `++genevolution_config.comparison_key=completion`

The `completion` key in `livecodebench` outputs contains just the extracted code from the generated solutions. For coding tasks, we empirically find that representing the candidate solution with just the extracted code to perform better than representing it with the text around it.


## Papers

- [GenSelect: A Generative Approach to Best-of-N](https://arxiv.org/abs/2507.17797)
- [AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset](https://arxiv.org/abs/2504.16891)


If you find GenSelect useful, please consider citing us!

```bibtex

@article{toshniwal2025genselect,
  title={{GenSelect: A Generative Approach to Best-of-N}},
  author={Shubham Toshniwal and Ivan Sorokin and Aleksander Ficek and Ivan Moshkov and Igor Gitman},
  year={2025},
  journal = {arXiv preprint arXiv:2507.17797},
}

@article{moshkov2025aimo2,
  title   = {{AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset}},
  author  = {Ivan Moshkov and Darragh Hanley and Ivan Sorokin and Shubham Toshniwal and Christof Henkel and Benedikt Schifferer and Wei Du and Igor Gitman},
  year    = {2025},
  journal = {arXiv preprint arXiv:2504.16891}
}
```