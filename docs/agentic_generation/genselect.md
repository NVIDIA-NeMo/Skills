# GenSelect


GenSelect is a generative Best-of-N method we introduced in the [OpenReasoning paper](https://arxiv.org/abs/2504.16891) followed by a more focused paper -- [GenSelect: A Generative Approach to Best-of-N](https://arxiv.org/abs/2507.17797). The method essentially uses an LLM to reason over and select the best candidate solution among the N candidates, leveraging LLMs' comparative strengths while scaling efficiently across parallel sampling budgets.


## Usage

We support GenSelect via the [generation pipeline](https://nvidia.github.io/NeMo-Skills/pipelines/generation/). To use genselect, just pass in `++genselect=True` when using the generation/eval pipelines.
We support both offline and online GenSelect:

- Offline mode: The N solutions/trajectories have already been generated and can be specified via `++genselect_config.generation_dir=<PATH_TO_GENERATED_DIR>`
- Online mode: The N solutions need to be generated as part of the generation job.


## Key Parameters

The GenSelect pipeline uses the same inference parameters as the generate pipeline. We allow overriding of three key inference config params:

- `temperature`: Inference temperature
- `tokens_to_generate`: Inference token budget
- `prompt_config`: Default config is `generic/genselect`

Other inference params like `top_p`, `min_p`, etc., inherit from the main generation configuration.

Next, we discuss GenSelect specific params. We first discuss parameters common to both the online and offline mode, and then discuss paramters specific to the offline mode.

### Common Parameters
- `window_size`: Number of solutions compared in a single GenSelect comparison (typically set to 8). Consider your model's context window size when setting this value (or allow for soft failure via `++server.enable_soft_fail=True`).
- `comparison_key`: The key from the generation output used for comparison (default: `generation`)

### Offline GenSelect Parameters
- `generation_dir`: The directory where the *offline* generated solutions are stored. We assume the solutions to be in `output-rs*.jsonl` files.
- `num_initial_solutions`: Number of solutions from the offline generated solutions that are used for GenSelect.


To specify these variables, say `window_size=16`, pass `++genselect_config.window_size=16` to the generate/eval pipelines.


## Sample Commands


### Online GenSelect

In this example, we show how to use GenSelect for [aime25](https://nvidia.github.io/NeMo-Skills/evaluation/natural-math/) with [Qwen/Qwen3-8B](https://huggingface.co/Qwen/Qwen3-8B).

```bash hl_lines="9"
ns eval \
  --benchmarks aime25 \
  --cluster local \
  --model Qwen/Qwen3-8B \
  --server_gpus 2 \
  --server_type vllm \
  --output_dir /experiments/qwen3_8b/genselect \
  ++inference.tokens_to_generate=16384 \
  ++genselect=True \
  ++server.enable_soft_fail=True \
  ++server.context_limit_retry_strategy=reduce_generation
```

The evaluation pipeline would first generate `window_size` solutions (8 by default), and then run GenSelect with these solutions in the prompt to pick one.
Note that the same model is being used for both solution generation and selection, which we refer to as **Self-GenSelect**.
Currently we only support Self-GenSelect in the online mode.


### Offline GenSelect

Offline genselect breaks down the generation and selection part into two separate steps. There are two use cases which are only possible with offline genselect:

- Using a different model for generation and selection.
- Using a processed version of generated output for comparison.

In the following example, we use [Qwen/Qwen3-8B](https://huggingface.co/Qwen/Qwen3-8B) to perform GenSelect over solutions generated by `Qwen/Qwen3-4B` for livecodebench.

```bash hl_lines="9-11"
ns eval \
  --benchmarks livecodebench:8 \
  --cluster local \
  --model Qwen/Qwen3-8B \
  --server_gpus 2 \
  --server_type vllm \
  --output_dir /experiments/qwen3_4b/genselect_qwen3_8b \
  ++inference.tokens_to_generate=16384 \
  ++genselect=True \
  ++genselect_config.generation_dir=/experiments/qwen3_4b/eval-results/livecodebench \
  ++genselect_config.comparison_key=\"completion\" \
  ++server.enable_soft_fail=True \
  ++server.context_limit_retry_strategy=reduce_generation
```

There are three things we want to highlight in the above example:

- We run the GenSelect pipeline a total of 8 times (`livecodebench:8`) over the same set of solutions
- The pre-generated solutions are specified via: `++genselect_config.generation_dir=/experiments/qwen3_4b/eval-results/livecodebench`
- Instead of the usual `generation` key for comparison, we use `++genselect_config.comparison_key=\"completion\"`

The `completion` key in `livecodebench` outputs contains just the extracted code from the generated solutions. For coding tasks, we empirically find that representing the candidate solution with just the extracted code to perform better than representing it with the text around it.


## Papers

- [GenSelect: A Generative Approach to Best-of-N](https://arxiv.org/abs/2507.17797)
- [AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset](https://arxiv.org/abs/2504.16891)


If you find GenSelect useful, please consider citing us!

```bibtex

@article{toshniwal2025genselect,
  title={{GenSelect: A Generative Approach to Best-of-N}},
  author={Shubham Toshniwal and Ivan Sorokin and Aleksander Ficek and Ivan Moshkov and Igor Gitman},
  year={2025},
  journal = {arXiv preprint arXiv:2507.17797},
}

@article{moshkov2025aimo2,
  title   = {{AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset}},
  author  = {Ivan Moshkov and Darragh Hanley and Ivan Sorokin and Shubham Toshniwal and Christof Henkel and Benedikt Schifferer and Wei Du and Igor Gitman},
  year    = {2025},
  journal = {arXiv preprint arXiv:2504.16891}
}
```