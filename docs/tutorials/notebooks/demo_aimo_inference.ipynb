{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62aa155e-9613-4ca9-a88d-d11ce4ac8b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA H100 80GB HBM3 \n",
      "GPU 1: NVIDIA H100 80GB HBM3 \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import psutil\n",
    "import copy\n",
    "import time\n",
    "import signal\n",
    "import requests\n",
    "import warnings\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Generator, Union, Optional\n",
    "from IPython.display import FileLink, display\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import transformers\n",
    "from transformers.utils import logging\n",
    "from openai import Stream\n",
    "from nemo_skills.prompt.utils import get_prompt, CodeTags\n",
    "from nemo_skills.code_execution.sandbox import get_sandbox\n",
    "from nemo_skills.evaluation.math_grader import extract_answer\n",
    "from nemo_skills.inference.model.code_execution import CodeExecutionWrapper\n",
    "from nemo_skills.inference.model import get_code_execution_model, get_model\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "!nvidia-smi -L | cut -d '(' -f 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed515c34-98c2-4d46-b00a-239a3faa47b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"./\"\n",
    "MODEL_DIR_HF = f\"{BASE_DIR}/OpenMath-Nemotron-14B-kaggle\"\n",
    "MODEL_DIR_BF16 = f\"{BASE_DIR}/OpenMath-Nemotron-14B-kaggle-bf16-trtllm\"\n",
    "MODEL_DIR_FP8 = f\"{BASE_DIR}/OpenMath-Nemotron-14B-kaggle-fp8-trtllm\"\n",
    "MODEL_DIR_FP8_DRAFT = f\"{BASE_DIR}/OpenMath-Nemotron-14B-kaggle-fp8-redrafter-trtllm\"\n",
    "benchmark = {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bd2af26-291d-4d2a-a3f8-07b98354e047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_server(host, port, timeout=300, interval=1):\n",
    "    url = f\"http://{host}:{port}\"\n",
    "    start_time = time.time()\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.put(url)\n",
    "            if response.status_code != 403:\n",
    "                return True\n",
    "        except requests.RequestException:\n",
    "            if time.time() - start_time > timeout:\n",
    "                raise TimeoutError(\"Server did not respond within timeout period\")\n",
    "            time.sleep(interval)\n",
    "                \n",
    "def start_server(model_dir, port=5000):\n",
    "    host = \"127.0.0.1\"\n",
    "    cmd = (\n",
    "        f'python -m tensorrt_llm.commands.serve serve {model_dir} '\n",
    "        f'    --tokenizer {MODEL_DIR_HF}'\n",
    "        f'    --backend trt '\n",
    "        f'    --tp_size 2 '\n",
    "        f'    --kv_cache_free_gpu_memory_fraction 0.92 '\n",
    "        f'    --max_batch_size 12 '\n",
    "        f'    --host {host} '\n",
    "        f'    --port {port}'\n",
    "    )\n",
    "    print(f\"Starting server from {model_dir} at {host}:{port}\")\n",
    "    model_name = model_dir.split(\"/\")[-1]\n",
    "    log_path = Path(f\"{model_name}_server_logs.log\").resolve()\n",
    "    log_file = open(log_path, \"w\", buffering=1)\n",
    "    proc = subprocess.Popen(cmd, shell=True, stdout=log_file, stderr=subprocess.STDOUT, preexec_fn=os.setsid)\n",
    "    print(\"Waiting for server to be ready (might take a while) ...\")\n",
    "    wait_for_server(host, port)\n",
    "    print(\"Server ready!\")\n",
    "    return proc\n",
    "\n",
    "def kill_server(proc, port=5000):\n",
    "    os.killpg(proc.pid, signal.SIGTERM)  \n",
    "    time.sleep(10)\n",
    "\n",
    "    for proc in psutil.process_iter(['pid', 'name']):\n",
    "        for conn in proc.connections(kind='inet'):\n",
    "            if conn.laddr.port == port:\n",
    "                print(f\"Killing process {proc.info['name']} (PID: {proc.info['pid']}) running on port {port}\")\n",
    "                os.kill(proc.info['pid'], 9)\n",
    "                break\n",
    "\n",
    "    time.sleep(10)\n",
    "    print(f'Server closed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12559d30-38e8-4b1d-bfe9-67761dc53677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server from .//OpenMath-Nemotron-14B-kaggle-fp8-redrafter-trtllm at 127.0.0.1:5000\n",
      "Waiting for server to be ready (might take a while) ...\n",
      "Server ready!\n"
     ]
    }
   ],
   "source": [
    "# we keep a reference to the server process, so we can stop it if necessary\n",
    "server_process = start_server(MODEL_DIR_FP8_DRAFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c91d12a7-007d-4f9e-adce-d8c3a2a71efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_DIR_FP8)\n",
    "\n",
    "def consume_stream(stream: Union[Stream, Generator], thread_id=None):\n",
    "    \"\"\"Process a single stream and return concatenated text with timing.\"\"\"\n",
    "    start_time = time.time()\n",
    "    result = \"\"\n",
    "    time_to_first_token = None\n",
    "    try:\n",
    "        for chunk in stream:\n",
    "            if chunk['generation'] is not None:\n",
    "                result += chunk['generation']\n",
    "                if time_to_first_token is None:\n",
    "                    time_to_first_token = time.time() - start_time\n",
    "    except Exception as e:\n",
    "        pass    \n",
    "        \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    num_generated_tokens = len(tokenizer.encode(result))\n",
    "    return {\n",
    "        'result': result,\n",
    "        'total_time': total_time,\n",
    "        'thread_id': thread_id,\n",
    "        'time_to_first_token': time_to_first_token,\n",
    "        'num_tokens': num_generated_tokens,\n",
    "        'throughput': num_generated_tokens / total_time,\n",
    "    }\n",
    "\n",
    "def stream_generate(\n",
    "    code_exec_model: CodeExecutionWrapper,\n",
    "    prompts: list[str | dict],\n",
    "    code_begin: str | list[str],\n",
    "    code_end: str | list[str],\n",
    "    code_output_begin: str | list[str],\n",
    "    code_output_end: str | list[str],\n",
    "    code_output_format: str | list[str],\n",
    "    tokens_to_generate: int | list[int] = 512,\n",
    "    temperature: float | list[float] = 0.0,\n",
    "    top_p: float | list[float] = 0.95,\n",
    "    top_k: int | list[int] = 0,\n",
    "    min_p: float | list[float] = 0.0,\n",
    "    repetition_penalty: float | list[float] = 1.0,\n",
    "    random_seed: int | list[int] = 0,\n",
    "    stop_phrases: list[str] | list[list[str]] | None = None,\n",
    "    remove_stop_phrases: bool = True,\n",
    "    timeout: int | list[int] | None = None,\n",
    "    max_code_executions: int | list[int] | None = None,\n",
    "    stop_after_n_completed : Optional[int] = None, \n",
    "    stop_after_n_seconds : Optional[int] = None,\n",
    "    stop_after_n_same_answer : Optional[int] = None,\n",
    "    return_stats : bool = False,\n",
    "    ) -> list[dict]:\n",
    "    \"\"\"Process multiple streams concurrently and return results with durations.\"\"\"\n",
    "\n",
    "    streams = code_exec_model.generate(\n",
    "        prompts=prompts,\n",
    "        code_begin=code_begin,\n",
    "        code_end=code_end,\n",
    "        code_output_begin=code_output_begin,\n",
    "        code_output_end=code_output_end,\n",
    "        code_output_format=code_output_format,\n",
    "        tokens_to_generate=tokens_to_generate,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        min_p=min_p,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        random_seed=random_seed,\n",
    "        stop_phrases=stop_phrases,\n",
    "        remove_stop_phrases=remove_stop_phrases,\n",
    "        timeout=timeout,\n",
    "        max_code_executions=max_code_executions,\n",
    "        stream=True,\n",
    "        )\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        # Submit all streams to thread pool with thread IDs\n",
    "        futures = [(thread_id, executor.submit(consume_stream, stream, thread_id)) for thread_id, stream in enumerate(streams)]\n",
    "        \n",
    "        if stop_after_n_completed is not None:\n",
    "            stop_after_n_completed = min(stop_after_n_completed, len(streams))\n",
    "        \n",
    "        start_time = time.time()\n",
    "        current_answers = [] # list of answers that have been completed\n",
    "        completed_futures = []  # list of tuples (thread_id, result_data)\n",
    "        \n",
    "        while futures:\n",
    "            time_elapsed  = time.time() - start_time\n",
    "            if stop_after_n_completed is not None and len(completed_futures) >= stop_after_n_completed:\n",
    "                print(f\"\\nStopping after {stop_after_n_completed} completed...\")\n",
    "                # This will asynchronously cancel all generations\n",
    "                # We don't break here because we want to collect results up to now\n",
    "                code_exec_model.model.cancel_all_generations()\n",
    "                print(f\"Canceling thread @ {time_elapsed:.2f} ({stop_after_n_completed} generations completed)\")\n",
    "                \n",
    "            elif stop_after_n_seconds is not None and time_elapsed >= stop_after_n_seconds:\n",
    "                print(f\"\\nStopping after {stop_after_n_seconds} seconds...\")\n",
    "                code_exec_model.model.cancel_all_generations()\n",
    "                print(f\"Canceling thread @ {time_elapsed:.2f} (maximum time reached)\")\n",
    "            \n",
    "            elif stop_after_n_same_answer is not None:\n",
    "                # Check whether at least n elements in current_answers are the same\n",
    "                if len(completed_futures) - len(set(current_answers)) >= stop_after_n_same_answer-1:\n",
    "                    print(f\"\\nStopping after {stop_after_n_same_answer} identical answers. {current_answers}\")\n",
    "                    code_exec_model.model.cancel_all_generations()\n",
    "                    print(f\"Canceling thread @ {time_elapsed:.2f} ({stop_after_n_same_answer} identical answers)\")\n",
    "            time.sleep(0.1)\n",
    "            \n",
    "            completed_in_this_iteration = []\n",
    "            for idx, future in futures:\n",
    "                if future.done():\n",
    "                    result_data = future.result()\n",
    "                    completed_futures.append((idx, result_data))\n",
    "                    completed_in_this_iteration.append((idx, future))\n",
    "                    current_answers.append(extract_answer(result_data['result']))\n",
    "\n",
    "            \n",
    "            for item in completed_in_this_iteration:\n",
    "                futures.remove(item)\n",
    "    \n",
    "    # Sort by original index and return results with durations\n",
    "    completed_futures.sort(key=lambda x: x[0])\n",
    "    if return_stats:\n",
    "        return [result_data for _, result_data in completed_futures]\n",
    "    else:\n",
    "        return [result_data['result'] for _, result_data in completed_futures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f245acb0-d318-485d-bf02-158d4b98ba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "sandbox = get_sandbox()  # localhost by default\n",
    "llm = get_code_execution_model(server_type=\"trtllm-serve\", sandbox=sandbox)\n",
    "\n",
    "# Initialize the prompt template\n",
    "prompt_template = get_prompt('generic/math', 'qwen-instruct')\n",
    "\n",
    "# Set the code tags directly on the config's code_tags attribute\n",
    "prompt_template.config.code_tags = CodeTags(\n",
    "    code_begin=\"<tool_call>\\n\",\n",
    "    code_end=\"</tool_call>\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e42b7f11-13e9-496b-85c8-590ec8bad37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = {\n",
    "    \"tokens_to_generate\": 12000,\n",
    "    \"temperature\": 0.,\n",
    "    \"top_k\": 20,\n",
    "    \"top_p\": 0.8,\n",
    "    \"repetition_penalty\": 1.0,\n",
    "    \"max_code_executions\": 2\n",
    "}\n",
    "\n",
    "problem = r'''\n",
    "The Fibonacci numbers are defined as follows: $F_0 = 0$, $F_1 = 1$, and $F_{n+1} = F_n + F_{n-1}$ for $n \\geq 1$. \n",
    "There are $N$ positive integers $n$ strictly less than $10^{101}$ such that $n^2 + (n+1)^2$ is a multiple of 5 but $F_{n-1}^2 + F_n^2$ is not. \n",
    "How many prime factors does $N$ have, counted with multiplicity?\n",
    "'''\n",
    "\n",
    "request = copy.deepcopy(sampling_params)\n",
    "list_of_texts =  [prompt_template.fill({'problem': problem})] * 12\n",
    "request[\"prompts\"] = list_of_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29efd899-095c-4fa0-af7f-f113b8402264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\n<|im_end|>\\n<|im_start|>user\\nSolve the following math problem. Make sure to put the answer (and only answer) inside \\\\boxed{}.\\n\\n\\nThe Fibonacci numbers are defined as follows: $F_0 = 0$, $F_1 = 1$, and $F_{n+1} = F_n + F_{n-1}$ for $n \\\\geq 1$. \\nThere are $N$ positive integers $n$ strictly less than $10^{101}$ such that $n^2 + (n+1)^2$ is a multiple of 5 but $F_{n-1}^2 + F_n^2$ is not. \\nHow many prime factors does $N$ have, counted with multiplicity?\\n<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4a64701-bf35-445d-a847-47add47ac378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stats_table(model_results):\n",
    "    \"\"\" Plots a table of statistics for the given model results. \"\"\"\n",
    "    data = {'Metric': ['Total Generation Time', 'Batch Throughput (Tok/sec)',  \"Avg Request Throughput (Tok/sec)\"]}\n",
    "    \n",
    "    for model_name in model_results:\n",
    "        granular_res = model_results[model_name]['results']\n",
    "        total_time = model_results[model_name]['total_time']\n",
    "        per_sample_throughputs = [r['throughput'] for r in granular_res]\n",
    "        \n",
    "        avg_per_sample_throughput = np.mean(per_sample_throughputs)\n",
    "        std_per_sample_throughput = np.std(per_sample_throughputs)\n",
    "\n",
    "        throughput = sum([r['num_tokens'] for r in granular_res]) / total_time\n",
    "        \n",
    "        data[model_name] = [\n",
    "            f\"{total_time:.1f}\",\n",
    "            f\"{throughput:.1f}\",\n",
    "            f\"{avg_per_sample_throughput:.1f} ± {std_per_sample_throughput:.2f}\",\n",
    "        ]\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ded5a6a-48d2-4602-b013-d1fa1e1a0933",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "results_fp8_draft = stream_generate(\n",
    "    llm,\n",
    "    **request,\n",
    "    **prompt_template.get_code_execution_args(),\n",
    "    stop_after_n_completed=8,\n",
    "    stop_after_n_seconds=200,\n",
    "    return_stats=True,\n",
    "    )\n",
    "total_time = time.time() - start_time\n",
    "benchmark['fp8_draft'] = {'results': results_fp8_draft, 'total_time': total_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b71f3aee-0639-48cc-8443-e42993f777fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server closed.\n",
      "Starting server from .//OpenMath-Nemotron-14B-kaggle-fp8-trtllm at 127.0.0.1:5000\n",
      "Waiting for server to be ready (might take a while) ...\n",
      "Server ready!\n"
     ]
    }
   ],
   "source": [
    "kill_server(server_process)\n",
    "server_process= start_server(MODEL_DIR_FP8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c8d3647-750d-48a0-9809-75872e95ef91",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "results_fp8 = stream_generate(\n",
    "    llm,\n",
    "    **request,\n",
    "    **prompt_template.get_code_execution_args(),\n",
    "    stop_after_n_completed=8,\n",
    "    stop_after_n_seconds=200,\n",
    "    return_stats=True,\n",
    "    )\n",
    "total_time = time.time() - start_time\n",
    "benchmark['fp8'] = {'results': results_fp8, 'total_time': total_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18be4322-910e-4907-9ae9-b52b514587c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server closed.\n",
      "Starting server from .//OpenMath-Nemotron-14B-kaggle-bf16-trtllm at 127.0.0.1:5000\n",
      "Waiting for server to be ready (might take a while) ...\n",
      "Server ready!\n"
     ]
    }
   ],
   "source": [
    "kill_server(server_process)\n",
    "server_process= start_server(MODEL_DIR_BF16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfd8439f-3997-4c9c-b275-fab87897f913",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "results_bf16 = stream_generate(\n",
    "    llm,\n",
    "    **request,\n",
    "    **prompt_template.get_code_execution_args(),\n",
    "    stop_after_n_completed=8,\n",
    "    stop_after_n_seconds=200,\n",
    "    return_stats=True,\n",
    "    )\n",
    "total_time = time.time() - start_time\n",
    "benchmark['bf16'] = {'results': results_bf16, 'total_time': total_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "364107a4-63cc-4b72-9b25-c6682570a43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Metric     fp8_draft          fp8         bf16\n",
      "0             Total Generation Time          33.8         72.9        170.4\n",
      "1        Batch Throughput (Tok/sec)        2036.0       1029.3        518.6\n",
      "2  Avg Request Throughput (Tok/sec)  175.7 ± 0.94  89.9 ± 0.99  44.1 ± 0.26\n"
     ]
    }
   ],
   "source": [
    "plot_stats_table(benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff781e42-5eb5-4b0e-a705-184e3e52ed13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
